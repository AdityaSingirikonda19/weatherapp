{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaSingirikonda19/weatherapp/blob/main/Copy_of_Final_Plotted_GlobalWildfireSpreadPredictionCNNLTSM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hF6UFOPUb6D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RX7mYGSB1g0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRloC7ioUmK6"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWA1SnUXUnPG"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle/ && cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEGEPI0kU5X2"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d fantineh/next-day-wildfire-spread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM3o9WdoU7HE"
      },
      "outputs": [],
      "source": [
        "!unzip /content/next-day-wildfire-spread.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg__CKnlU7nA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxsILo-2U953"
      },
      "outputs": [],
      "source": [
        "train_file_pattern = '/content/next_day_wildfire_spread_train*'\n",
        "val_file_pattern = '/content/next_day_wildfire_spread_eval*'\n",
        "test_file_pattern = '/content/next_day_wildfire_spread_test*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYycWzWCVBnH"
      },
      "outputs": [],
      "source": [
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask', ]\n",
        "\n",
        "DATA_STATS = {'elevation': (0.0, 3141.0, 657.3003, 649.0147), 'pdsi': (-6.1298, 7.8760, -0.0053, 2.6823), 'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677), 'pr': (0.0, 44.5304, 1.7398051, 4.4828), 'sph': (0., 1., 0.0071658953, 0.0042835088), 'th': (0., 360.0, 190.3298, 72.5985), 'tmmn': (253.15, 298.9489, 281.08768, 8.9824), 'tmmx': (253.15, 315.0923, 295.17383, 9.8155), 'vs': (0.0, 10.0243, 3.8501, 1.4110), 'erc': (0.0, 106.2489, 37.3263, 20.8460), 'population': (0., 2534.0630, 25.5314, 154.7233), 'PrevFireMask': (-1., 1., 0., 1.), 'FireMask': (-1., 1., 0., 1.)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBVXb4sWVvPu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU9yFmT9Wa_8"
      },
      "outputs": [],
      "source": [
        "def input_output_images(input_img:tf.Tensor, output_img:tf.Tensor, sample_size:int, num_in_channels:int, num_out_channels:int):\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(combined,[sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "  #return combined\n",
        "#input_output_images(input_img, output_img, sample_size, num_in_channels, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BysQjFZb18t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_crop_input_and_output_images(input_img:tf.Tensor, output_img:tf.Tensor, sample_size:int, num_in_channels:int, num_out_channels:int,) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(\n",
        "      combined,\n",
        "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def center_crop_input_and_output_images(input_img: tf.Tensor, output_img: tf.Tensor, sample_size: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  central_fraction = sample_size / input_img.shape[0]\n",
        "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
        "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
        "  return input_img, output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSMBb9r1bcLu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _get_base_key(key: Text) -> Text:\n",
        "  match = re.match(r'[a-zA-Z]+', key)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  raise ValueError(\n",
        "      f'The provided key does not match the expected pattern: {key}')\n",
        "\n",
        "\n",
        "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
        "\n",
        "\n",
        "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  inputs = inputs - mean\n",
        "  return tf.math.divide_no_nan(inputs, std)\n",
        "\n",
        "def _get_features_dict(\n",
        "    sample_size: int,\n",
        "    features: List[Text],\n",
        ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
        "  sample_shape = [sample_size, sample_size]\n",
        "  features = set(features)\n",
        "  columns = [\n",
        "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
        "      for _ in features\n",
        "  ]\n",
        "  return dict(zip(features, columns))\n",
        "\n",
        "\n",
        "def _parse_fn(example_proto: tf.train.Example, data_size: int, sample_size: int, num_in_channels: int, clip_and_normalize: bool, clip_and_rescale: bool, random_crop: bool, center_crop: bool,) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  if (random_crop and center_crop):\n",
        "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
        "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
        "  feature_names = input_features + output_features\n",
        "  features_dict = _get_features_dict(data_size, feature_names)\n",
        "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "\n",
        "  if clip_and_normalize:\n",
        "    inputs_list = [_clip_and_normalize(features.get(key), key) for key in input_features]\n",
        "  elif clip_and_rescale:\n",
        "    inputs_list = [_clip_and_rescale(features.get(key), key) for key in input_features]\n",
        "  else:\n",
        "    inputs_list = [features.get(key) for key in input_features]\n",
        "\n",
        "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
        "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
        "\n",
        "  outputs_list = [features.get(key) for key in output_features]\n",
        "  assert outputs_list, 'outputs_list should not be empty'\n",
        "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
        "\n",
        "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
        "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3''but dimensions of outputs_stacked'f' are {outputs_stacked_shape}')\n",
        "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
        "\n",
        "  if random_crop:\n",
        "    input_img, output_img = random_crop_input_and_output_images(input_img, output_img, sample_size, num_in_channels, 1)\n",
        "  if center_crop:\n",
        "    input_img, output_img = center_crop_input_and_output_images(input_img, output_img, sample_size)\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern: Text, data_size: int, sample_size: int, batch_size: int, num_in_channels: int, compression_type: Text, clip_and_normalize: bool, clip_and_rescale: bool, random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "  if (clip_and_normalize and clip_and_rescale):\n",
        "    raise ValueError('Cannot have both normalize and rescale.')\n",
        "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "  dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(lambda x: _parse_fn(x, data_size, sample_size, num_in_channels, clip_and_normalize, clip_and_rescale, random_crop, center_crop),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13_rX0Z8c3BP"
      },
      "outputs": [],
      "source": [
        "train_dataset = get_dataset(\n",
        "  train_file_pattern,\n",
        "  data_size=64,\n",
        "  sample_size=32,\n",
        "  batch_size=100,\n",
        "  num_in_channels=12,\n",
        "  compression_type=None,\n",
        "  clip_and_normalize=False,\n",
        "  clip_and_rescale=False,\n",
        "  random_crop=True,\n",
        "  center_crop=False\n",
        ")\n",
        "val_dataset = get_dataset(\n",
        "    val_file_pattern,\n",
        "    data_size=64,\n",
        "    sample_size=32,\n",
        "    batch_size=100,\n",
        "    num_in_channels=12,\n",
        "    compression_type=None,\n",
        "    clip_and_normalize=False,\n",
        "    clip_and_rescale=False,\n",
        "    random_crop=True,\n",
        "    center_crop=False\n",
        ")\n",
        "test_dataset = get_dataset(\n",
        "    test_file_pattern,\n",
        "    data_size=64,\n",
        "    sample_size=32,\n",
        "    batch_size=100,\n",
        "    num_in_channels=12,\n",
        "    compression_type=None,\n",
        "    clip_and_normalize=False,\n",
        "    clip_and_rescale=False,\n",
        "    random_crop=True,\n",
        "    center_crop=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXWzl7UwSDCk"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "  def __init__(self, logs={}):\n",
        "    self.logs=[]\n",
        "  def on_epoch_begin(self, epoch, logs={}):\n",
        "    self.starttime = timer()\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    self.logs.append(timer()-self.starttime)\n",
        "tcb = TimingCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rVVvM19QijX"
      },
      "outputs": [],
      "source": [
        "# Define some callbacks to improve training.\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor = 'val_auc', verbose = 1, save_best_only = False, period = 1)\n",
        "\n",
        "def algo_train():\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "  # Define modifiable training hyperparameters.\n",
        "  epochs = 20\n",
        "  batch_size = 100\n",
        "\n",
        "  # Fit the model to the training data.\n",
        "  history = model.fit(\n",
        "      features_np,\n",
        "      firemask_np,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=(features_val_np,firemask_val_np),\n",
        "      callbacks=[early_stopping, reduce_lr, tcb, checkpoint]\n",
        "  )\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LasGGLjaQhjS"
      },
      "outputs": [],
      "source": [
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(1, 32, 32, 12))\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv3D(\n",
        "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),metrics=[tf.keras.metrics.AUC()]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L69wHrCEjw0A"
      },
      "outputs": [],
      "source": [
        "weights_auc = []\n",
        "weights_loss = []\n",
        "\n",
        "def add_weights(history, sum_auc, sum_loss):\n",
        "  weights = history.history\n",
        "  result = weights.items()\n",
        "  data = list(result)\n",
        "  data_np = np.array(data)\n",
        "  for x in range(len(data_np[0][1])):\n",
        "    sum_auc = sum_auc + data_np[1][1][x]\n",
        "    sum_loss = sum_loss + data_np[0][1][x]\n",
        "  weights_auc.append(sum_auc/len(data_np[0][1]))\n",
        "  weights_loss.append(sum_loss/len(data_np[0][1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IadQWHX22tch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2eXswj7O9yd"
      },
      "outputs": [],
      "source": [
        "global sum_auc, sum_loss\n",
        "sum_auc = 0\n",
        "sum_loss = 0\n",
        "train_iter = iter(train_dataset)\n",
        "val_iter = iter(val_dataset)\n",
        "for i in range(150):\n",
        "  # features, firemask = next(iter(train_dataset))\n",
        "  features, firemask = next(train_iter)\n",
        "  features_np = features.numpy()\n",
        "  firemask_np = firemask.numpy()\n",
        "  features_np = np.expand_dims(features_np, axis = 1)\n",
        "  firemask_np = np.expand_dims(firemask_np, axis = 1)\n",
        "  # features_val, firemask_val = next(iter(val_dataset))\n",
        "  features_val, firemask_val = next(val_iter)\n",
        "  features_val_np = features_val.numpy()\n",
        "  firemask_val_np = firemask_val.numpy()\n",
        "  features_val_np = np.expand_dims(features_val_np, axis = 1)\n",
        "  firemask_val_np = np.expand_dims(firemask_val_np, axis = 1)\n",
        "  history = algo_train()\n",
        "  #model.save('drive.google.com/drive/folders/1jcYiH6yeu9H2TRSQXC5SdHJX2B5pMFoS?usp=sharing')\n",
        "  print(\"ITERATION: \", i+1)\n",
        "  add_weights(history, sum_auc, sum_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWBTFlHss7_D"
      },
      "outputs": [],
      "source": [
        "weights_auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoniJn1atqcd"
      },
      "outputs": [],
      "source": [
        "weights_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWgpsTKhKvQC"
      },
      "outputs": [],
      "source": [
        "iteration_arr = []\n",
        "\n",
        "for i in range(150):\n",
        "  iteration_arr.append(i+1)\n",
        "\n",
        "plt.plot(iteration_arr, weights_auc)\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.title(\"AUC Plot\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N7XN6m8yNV-"
      },
      "outputs": [],
      "source": [
        "plt.plot(iteration_arr, weights_loss, 'r')\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6e4R1vKTmn9"
      },
      "outputs": [],
      "source": [
        "print(f\"Average time per epoch: {sum(tcb.logs)/len(tcb.logs)} seconds\")\n",
        "print(f\"Total Time taken: {sum(tcb.logs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ZK4zsHQf2d"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#model = Sequential()\n",
        "#model.add(Dense(2, input_dim=1, activation='relu'))\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir = 'TB', expand_nested = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = firemask_np[9][0].reshape(32,32)\n",
        "plt.imshow(a, cmap=\"Set1_r\")"
      ],
      "metadata": {
        "id": "oNTT8z4O1jCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}